# -*- coding: utf-8 -*-
"""Aura+Ego.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b5FBhEDstY_k6ht2XeUn_D8nlDzIjK5f
"""



"""## Importar bibliotecas

### Subtask:
Importar as bibliotecas necessárias para manipulação de dados, machine learning e visualização.

**Reasoning**:
Import necessary libraries for data manipulation, machine learning, and visualization as specified in the instructions.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.preprocessing import StandardScaler, LabelEncoder

"""## Baixar e carregar dados

### Subtask:
Baixar o dataset Kepler Objects of Interest (KOI) da NASA e carregar os dados em um pandas DataFrame.

**Reasoning**:
Given the difficulties in programmatic download, the most reliable approach is to assume the user has uploaded the 'koi-cumulative.csv' file. This step loads the data from the local file.
"""

# Assumindo que o arquivo 'koi-cumulative.csv' esteja disponível no ambiente atual.
# Carregar o conjunto de dados
try:
    df = pd.read_csv('koi-cumulative.csv', comment='#')
    print("Conjunto de dados carregado com sucesso do arquivo local.")
    display(df.head())
    display(df.info())

except FileNotFoundError:
    print("Erro: 'koi-cumulative.csv' não encontrado. Por favor, certifique-se de que o arquivo esteja disponível no ambiente.")
    df = None # Definir df como None em caso de erro
except Exception as e:
    print(f"Ocorreu um erro inesperado durante o carregamento do conjunto de dados: {e}")
    df = None # Definir df como None em caso de erro

"""## Pré-processar dados

### Subtask:
Pré-processar os dados para prepará-los para o treinamento do modelo. Isso inclui selecionar as features relevantes, tratar valores ausentes, normalizar os dados e codificar a variável target.

**Reasoning**:
Process the loaded DataFrame by selecting relevant features, handling missing values, encoding the target variable, and normalizing numerical features to prepare the data for machine learning models.
"""

# Verifica se o DataFrame original foi carregado com sucesso
if df is not None:
    print("Iniciando pré-processamento dos dados...")

    # 1. Definir as colunas relevantes para o treinamento do modelo.
    # Selecionamos features que são importantes para a classificação de exoplanetas,
    # incluindo flags de falso positivo e características do trânsito e da estrela.
    selected_features = [
        'koi_fpflag_nt', 'koi_fpflag_ss', 'koi_fpflag_co', 'koi_fpflag_ec', # Flags de Falso Positivo (Non-transit, Stellar, Centroid Offset, Ephemeris)
        'koi_period',    # Período orbital (dias)
        'koi_time0bk',   # Época de trânsito (tempo do primeiro trânsito)
        'koi_impact',    # Parâmetro de impacto (distância do centro do disco estelar)
        'koi_duration',  # Duração do trânsito (horas)
        'koi_depth',     # Profundidade do trânsito (milhares da razão de fluxo)
        'koi_prad',      # Raio planetário (em raios terrestres)
        'koi_teq',       # Temperatura de equilíbrio (K)
        'koi_insol',     # Fluxo de insolação (terra=1)
        'koi_model_snr', # Transit Model SNR (razão sinal/ruído do modelo de trânsito)
        'koi_steff',     # Temperatura efetiva estelar (K)
        'koi_slogg',     # Gravidade superficial estelar (log10(cm/s^2))
        'koi_srad',      # Raio estelar (em raios solares)
        'ra',            # Ascensão reta (graus)
        'dec',           # Declinação (graus)
        'kepmag'         # Magnitude Kepler (magnitude aparente na banda Kepler)
    ]

    # A variável target é a coluna que queremos prever
    target_variable = 'koi_disposition' # Classes: 'CONFIRMED', 'CANDIDATE', 'FALSE POSITIVE'

    # Combinar as listas de features e target para verificar a existência no DataFrame original
    all_selected_columns = selected_features + [target_variable]
    # Filtrar as colunas que realmente existem no DataFrame carregado
    existing_columns = [col for col in all_selected_columns if col in df.columns]

    # Verificar se alguma coluna selecionada não foi encontrada
    if len(existing_columns) != len(all_selected_columns):
        missing_columns = list(set(all_selected_columns) - set(existing_columns))
        print(f"Aviso: As seguintes colunas selecionadas não foram encontradas no DataFrame: {missing_columns}")
        # Atualizar a lista de features selecionadas para incluir apenas as colunas existentes (excluindo a target se ela estiver faltando)
        selected_features = [col for col in selected_features if col in existing_columns and col != target_variable]

        # Se a variável target estiver faltando, emitir um erro e não prosseguir
        if target_variable not in existing_columns:
            print(f"Erro: A variável target '{target_variable}' não foi encontrada no DataFrame. Não é possível prosseguir com o pré-processamento.")
            df_processed = None # Indicar falha ao criar o DataFrame processado
        else:
            # Criar um novo DataFrame contendo apenas as colunas de feature existentes e a variável target.
            df_processed = df[existing_columns].copy()
            print("Novo DataFrame criado com as colunas existentes selecionadas.")
    else:
        # Se todas as colunas selecionadas existirem, criar o DataFrame processado com todas elas.
        df_processed = df[all_selected_columns].copy()
        print("Novo DataFrame criado com as colunas selecionadas.")

    # Continuar com o pré-processamento somente se o DataFrame processado foi criado com sucesso
    if df_processed is not None:
        print("\nTratando valores ausentes...")
        # 3. Tratar valores ausentes no novo DataFrame.
        # Identificar colunas numéricas para imputação
        numerical_cols = df_processed.select_dtypes(include=np.number).columns.tolist()
        # Remover a variável target da lista de colunas numéricas para imputação, se ela for numérica
        if target_variable in numerical_cols:
            numerical_cols.remove(target_variable)

        # Preencher valores numéricos ausentes com a média das respectivas colunas
        for col in numerical_cols:
            # Usamos .loc para evitar o FutureWaarning sobre inplace=True em cópias
            df_processed.loc[:, col] = df_processed.loc[:, col].fillna(df_processed.loc[:, col].mean())

        # Verificar se ainda existem valores ausentes após a imputação numérica (pode haver em colunas não numéricas, se houver)
        if df_processed.isnull().sum().sum() > 0:
             print("Aviso: Ainda existem valores ausentes após preencher as colunas numéricas.")
             # Exibir a contagem de valores ausentes por coluna para investigação
             display(df_processed.isnull().sum())

        print("Tratamento de valores ausentes concluído.")

        print("\nCodificando a variável target...")
        # 4. Codificar a variável target 'koi_disposition' em rótulos numéricos.
        # Isso é necessário porque modelos de machine learning geralmente trabalham com valores numéricos.
        if target_variable in df_processed.columns:
            label_encoder = LabelEncoder()
            # Realiza a codificação da coluna target
            df_processed[target_variable] = label_encoder.fit_transform(df_processed[target_variable])
            print(f"Variável target '{target_variable}' codificada.")
            # Mostra o mapeamento das classes originais para os rótulos numéricos
            print(f"Classes originais e seus rótulos codificados: {list(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))}")
        else:
             # Se a variável target não estiver no DataFrame processado (o que não deve acontecer se a verificação anterior funcionou), emitir erro.
             print(f"Erro: A variável target '{target_variable}' não foi encontrada no DataFrame processado. Não é possível codificar.")
             df_processed = None # Indicar falha se a target estiver faltando


    # Continuar com a normalização somente se o DataFrame processado ainda for válido
    if df_processed is not None:
        print("\nNormalizando features numéricas...")
        # 5. Identificar as colunas de feature numéricas no DataFrame processado para normalização.
        # Excluir a variável target das colunas numéricas para normalização, pois ela já foi codificada.
        numerical_feature_cols = df_processed[selected_features].select_dtypes(include=np.number).columns.tolist()

        # 6. Normalizar as colunas de feature numéricas usando StandardScaler.
        # StandardScaler remove a média e escala para a variância unitária,
        # o que é importante para muitos algoritmos de ML que são sensíveis à escala das features.
        if numerical_feature_cols:
            scaler = StandardScaler()
            # Aplica a normalização às colunas numéricas selecionadas
            df_processed[numerical_feature_cols] = scaler.fit_transform(df_processed[numerical_feature_cols])
            print("Features numéricas normalizadas usando StandardScaler.")
        else:
            print("Nenhuma feature numérica para normalizar.")

        # 7. Exibir o cabeçalho e as informações do DataFrame pré-processado final.
        print("\nDataFrame Pré-processado Finalizado:")
        display(df_processed.head())
        display(df_processed.info())

else:
    # Mensagem se o DataFrame original não foi carregado (por exemplo, arquivo não encontrado)
    print("Pulando etapa de pré-processamento pois o DataFrame original não foi carregado.")
    df_processed = None # Garantir que df_processed seja None se df for None

"""## Separar dados e Treinar modelos

### Subtask:
Dividir o dataset pré-processado em conjuntos de treino e teste de forma estratificada e, em seguida, treinar um Random Forest Classifier e um XGBoost Classifier nos dados de treino.

**Reasoning**:
Combine the data splitting and model training steps to ensure that the training data (`X_train`, `y_train`) is available immediately before the models are trained, addressing previous `NameError` issues and maintaining logical flow.
"""

# Verifica se o DataFrame pré-processado existe antes de continuar
if df_processed is None:
    print("Erro: O DataFrame pré-processado `df_processed` não existe. Não é possível separar os dados e treinar os modelos.")
else:
    print("Iniciando separação dos dados e treinamento dos modelos...")

    # 1. Separe as features (X) da variável target (y).
    # X conterá todas as colunas de features, e y conterá apenas a coluna target.
    target_variable = 'koi_disposition' # A variável target definida na etapa de pré-processamento

    # Verifica se a coluna 'koi_disposition' existe em `df_processed`.
    # Esta verificação é redundante se o pré-processamento foi bem sucedido, mas adiciona robustez.
    if target_variable not in df_processed.columns:
        print(f"Erro: A variável target '{target_variable}' não foi encontrada no DataFrame pré-processado. Não é possível separar os dados e treinar os modelos.")
    else:
        X = df_processed.drop(columns=[target_variable]) # Remove a coluna target para criar o conjunto de features
        y = df_processed[target_variable] # Seleciona apenas a coluna target

        # 2. Utilize a função `train_test_split` para dividir os dados.
        # Dividimos os dados em conjuntos de treino e teste para avaliar o desempenho do modelo em dados não vistos.
        # test_size=0.2 define que 20% dos dados serão usados para teste.
        # random_state=42 garante que a divisão será a mesma a cada execução para reprodutibilidade.
        # stratify=y garante que a proporção das classes na variável target (y) seja a mesma nos conjuntos de treino e teste.
        print("Dividindo dados em conjuntos de treino e teste (80/20, estratificado)...")
        X_train, X_test, y_train, y_test = train_test_split(
            X, y,
            test_size=0.2,
            random_state=42,
            stratify=y
        )

        # 3. Imprima as formas (`shape`) de cada um dos conjuntos resultantes.
        # Isso confirma que a divisão ocorreu corretamente e mostra o número de amostras e features em cada conjunto.
        print("Dados divididos em conjuntos de treino e teste de forma estratificada.")
        print("Forma de X_train:", X_train.shape)
        print("Forma de X_test:", X_test.shape)
        print("Forma de y_train:", y_train.shape)
        print("Forma de y_test:", y_test.shape)

        # 4. Inicializar e treinar o modelo Random Forest Classifier.
        # Random Forest é um modelo de ensemble baseado em árvores de decisão.
        # n_estimators=100: Define o número de árvores na floresta.
        # random_state=42: Para reprodutibilidade do treinamento.
        print("\nTreinando Random Forest Classifier...")
        rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
        rf_model.fit(X_train, y_train) # Treina o modelo com os dados de treino
        print("Random Forest Classifier treinado.")

        # 5. Inicializar e treinar o modelo XGBoost Classifier.
        # XGBoost (Extreme Gradient Boosting) é outro modelo de ensemble poderoso.
        # use_label_encoder=False e eval_metric='mlogloss' são parâmetros comuns para evitar warnings e especificar a métrica de avaliação.
        # random_state=42: Para reprodutibilidade do treinamento.
        print("\nTreinando XGBoost Classifier...")
        xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)
        xgb_model.fit(X_train, y_train) # Treina o modelo com os dados de treino
        print("XGBoost Classifier treinado.")

print("\nEtapa de separação de dados e treinamento de modelos concluída.")

"""## Avaliar modelos

### Subtask:
Avaliar o desempenho dos modelos nos dados de teste utilizando métricas como Acurácia, Precisão, Recall e F1-score, e gerar a matriz de confusão.

**Reasoning**:
Evaluate the performance of the trained Random Forest and XGBoost models on the test set (`X_test`, `y_test`) using key classification metrics and generate confusion matrices to understand their performance in detail. This step follows directly after model training.
"""

# Verifica se os modelos e dados de teste estão disponíveis antes de avaliar
if 'rf_model' in locals() and 'xgb_model' in locals() and 'X_test' in locals() and 'y_test' in locals():
    print("Avaliando modelos...")

    # Passo 1: Fazer previsões nos dados de teste para ambos os modelos.
    # Usamos os modelos treinados para prever a classe dos dados no conjunto de teste.
    y_pred_rf = rf_model.predict(X_test)
    y_pred_xgb = xgb_model.predict(X_test)

    # Passo 2: Calcular métricas de avaliação para o Random Forest.
    print("\nMétricas de Avaliação para Random Forest:")
    # Acurácia: Proporção de previsões corretas.
    accuracy_rf = accuracy_score(y_test, y_pred_rf)
    # Precisão (weighted): Capacidade do modelo de não rotular como positivo uma amostra negativa. 'weighted' considera o suporte para cada classe.
    precision_rf = precision_score(y_test, y_pred_rf, average='weighted')
    # Recall (weighted): Capacidade do modelo de encontrar todas as amostras positivas. 'weighted' considera o suporte para cada classe.
    recall_rf = recall_score(y_test, y_pred_rf, average='weighted')
    # F1-score (weighted): Média harmônica entre Precisão e Recall. 'weighted' considera o suporte para cada classe.
    f1_rf = f1_score(y_test, y_pred_rf, average='weighted')

    print(f"Acurácia (Proporção de acertos totais): {accuracy_rf:.4f}")
    print(f"Precisão (weighted - Quão confiante o modelo está em suas previsões): {precision_rf:.4f}")
    print(f"Recall (weighted - Capacidade do modelo de encontrar todas as instâncias positivas): {recall_rf:.4f}")
    print(f"F1-score (weighted - Média harmônica entre Precisão e Recall): {f1_rf:.4f}")

    # Passo 3: Gerar e exibir a matriz de confusão para o Random Forest.
    print("\nMatriz de Confusão para Random Forest:")
    # A matriz de confusão mostra o número de acertos e erros do modelo para cada classe.
    # As linhas representam as classes reais, e as colunas representam as classes previstas.
    # Verifica se label_encoder está disponível para usar os nomes das classes originais nos rótulos.
    if 'label_encoder' in locals() and hasattr(label_encoder, 'classes_'):
        cm_rf = confusion_matrix(y_test, y_pred_rf)
        plt.figure(figsize=(8, 6))
        # Cria um heatmap da matriz de confusão para visualização
        sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
        plt.xlabel('Previsto pelo Modelo')
        plt.ylabel('Real (Verdadeiro)')
        plt.title('Matriz de Confusão - Random Forest')
        plt.show()
    else:
        print("Erro: label_encoder não está disponível ou não foi ajustado corretamente para gerar a matriz de confusão com rótulos.")


    # Passo 4: Calcular métricas de avaliação para o XGBoost.
    print("\nMétricas de Avaliação para XGBoost:")
    # Calcula as mesmas métricas para o modelo XGBoost
    accuracy_xgb = accuracy_score(y_test, y_pred_xgb)
    precision_xgb = precision_score(y_test, y_pred_xgb, average='weighted')
    recall_xgb = recall_score(y_test, y_pred_xgb, average='weighted')
    f1_xgb = f1_score(y_test, y_pred_xgb, average='weighted')

    print(f"Acurácia (Proporção de acertos totais): {accuracy_xgb:.4f}")
    print(f"Precisão (weighted - Quão confiante o modelo está em suas previsões): {precision_xgb:.4f}")
    print(f"Recall (weighted - Capacidade do modelo de encontrar todas as instâncias positivas): {recall_xgb:.4f}")
    print(f"F1-score (weighted - Média harmônica entre Precisão e Recall): {f1_xgb:.4f}")


    # Passo 5: Gerar e exibir a matriz de confusão para o XGBoost.
    print("\nMatriz de Confusão para XGBoost:")
     # Gera e exibe a matriz de confusão para o XGBoost
    if 'label_encoder' in locals() and hasattr(label_encoder, 'classes_'):
        cm_xgb = confusion_matrix(y_test, y_pred_xgb)
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
        plt.xlabel('Previsto pelo Modelo')
        plt.ylabel('Real (Verdadeiro)')
        plt.title('Matriz de Confusão - XGBoost')
        plt.show()
    else:
         print("Erro: label_encoder não está disponível ou não foi ajustado corretamente para gerar a matriz de confusão com rótulos.")

else:
    print("Erro: Os modelos, dados de teste ou ambos não estão disponíveis para avaliação. Por favor, execute as células anteriores.")

print("\nEtapa de avaliação de modelos concluída.")

"""## Visualizações Interativas com Plotly

### Subtask:
Criar visualizações interativas (Matriz de Confusão e Importância das Features) usando a biblioteca Plotly.

**Reasoning**:
Generate interactive versions of the confusion matrices and feature importance plots using Plotly. This enhances the visualizations for better exploration and understanding, especially for a lay audience, and prepares them for potential embedding in web applications.
"""

import plotly.express as px
import plotly.graph_objects as go
import pandas as pd # Importar pandas aqui para garantir que esteja disponível

# Dicionário com as descrições das features importantes fornecidas pelo usuário
# Isso ajuda a entender o significado de cada feature no contexto astronômico.
feature_descriptions = {
    'koi_fpflag_co': 'Indica se existe evidência de contaminação por objetos próximos (muito relevante para distinguir falsos positivos).',
    'koi_model_snr': 'Relação sinal/ruído do trânsito, fundamental para saber se a queda de luz é confiável.',
    'koi_fpflag_nt': 'Indica se foi marcada como falso positivo por uma estrela binária de nêutrons/companheira.',
    'koi_prad': 'Raio estimado do planeta - ajuda a distinguir candidatos plausíveis de falsos.',
    'koi_depth': 'Profundidade do trânsito - quedas muito rasas ou muito profundas podem indicar falsos positivos.',
    'koi_period': 'Período orbital - planetas tendem a ter padrões mais regulares.',
    'koi_impact': 'Parâmetro de impacto - geometria da órbita.',
    # Adicione descrições para outras features importantes se desejar
    'koi_fpflag_ss': 'Indica se foi marcada como falso positivo por um sistema estelar binário.',
    'koi_fpflag_ec': 'Indica se foi marcada como falso positivo por uma curva de luz eclipsante.',
    'koi_insol': 'Fluxo de insolação recebido pelo planeta.',
}


# --- Visualização Interativa da Matriz de Confusão (Random Forest) ---
print("Gerando Matriz de Confusão Interativa (Random Forest)...")
# Verifica se a matriz de confusão e o label encoder estão disponíveis
if 'cm_rf' in locals() and 'label_encoder' in locals() and hasattr(label_encoder, 'classes_'):
    # Converte a matriz de confusão para um DataFrame para facilitar a visualização com Plotly
    cm_rf_df = pd.DataFrame(cm_rf, index=label_encoder.classes_, columns=label_encoder.classes_)

    # Cria o heatmap interativo da matriz de confusão usando Plotly Express
    fig_cm_rf = px.imshow(cm_rf_df,
                          labels=dict(x="Previsto pelo Modelo", y="Real (Verdadeiro)", color="Contagem"), # Rótulos para os eixos e barra de cor
                          x=label_encoder.classes_, # Rótulos do eixo X (Previsto)
                          y=label_encoder.classes_, # Rótulos do eixo Y (Real)
                          text_auto=True, # Mostrar os valores dentro das células
                          color_continuous_scale='Blues', # Esquema de cores
                          title='Matriz de Confusão Interativa - Random Forest') # Título do gráfico

    # Atualiza o layout para garantir que os títulos dos eixos sejam claros
    fig_cm_rf.update_layout(
        xaxis_title='Previsto pelo Modelo',
        yaxis_title='Real (Verdadeiro)'
    )
    # Exibe o gráfico interativo
    fig_cm_rf.show()
else:
    print("Erro: Matriz de confusão do Random Forest ou label_encoder não estão disponíveis para gerar a visualização.")


# --- Visualização Interativa da Matriz de Confusão (XGBoost) ---
print("\nGerando Matriz de Confusão Interativa (XGBoost)...")
# Verifica se a matriz de confusão do XGBoost e o label encoder estão disponíveis
if 'cm_xgb' in locals() and 'label_encoder' in locals() and hasattr(label_encoder, 'classes_'):
    # Converte a matriz de confusão para um DataFrame
    cm_xgb_df = pd.DataFrame(cm_xgb, index=label_encoder.classes_, columns=label_encoder.classes_)

    # Cria o heatmap interativo para a matriz de confusão do XGBoost
    fig_cm_xgb = px.imshow(cm_xgb_df,
                          labels=dict(x="Previsto pelo Modelo", y="Real (Verdadeiro)", color="Contagem"),
                          x=label_encoder.classes_,
                          y=label_encoder.classes_,
                          text_auto=True,
                          color_continuous_scale='Blues',
                          title='Matriz de Confusão Interativa - XGBoost')

    # Atualiza o layout
    fig_cm_xgb.update_layout(
        xaxis_title='Previsto pelo Modelo',
        yaxis_title='Real (Verdadeiro)'
    )
    # Exibe o gráfico
    fig_cm_xgb.show()
else:
     print("Erro: Matriz de confusão do XGBoost ou label_encoder não estão disponíveis para gerar a visualização.")


# --- Visualização Interativa da Importância das Features (Random Forest) ---
print("\nGerando Gráfico Interativo de Importância das Features (Random Forest)...")
# Verifica se os dados de importância das features do Random Forest estão disponíveis
if 'sorted_feature_importances_rf' in locals():
    # Criar um DataFrame com as top 10 features e suas descrições para usar no gráfico
    top_10_rf = sorted_feature_importances_rf.head(10).reset_index()
    top_10_rf.columns = ['Feature', 'Importancia']
    # Mapeia as features para suas descrições, usando um fallback se a descrição não for encontrada
    top_10_rf['Descricao'] = top_10_rf['Feature'].map(feature_descriptions).fillna('Descrição não disponível')

    # Cria o gráfico de barras interativo usando Plotly Express
    fig_feat_rf = px.bar(top_10_rf,
                         x='Importancia', # Eixo X: o valor da importância
                         y='Feature',     # Eixo Y: o nome da feature
                         orientation='h', # Barras horizontais
                         labels={'Importancia':'Importância da Feature', 'Feature':'Feature'}, # Rótulos dos eixos
                         title='Top 10 Importância das Features Interativa - Random Forest', # Título do gráfico
                         hover_data={'Descricao': True, 'Importancia':':.4f'}) # Adiciona Descricao e Importancia formatada ao hover

    # Ordena as barras pelo valor da importância (ascendente para que a mais importante fique no topo com orientation='h')
    fig_feat_rf.update_layout(yaxis={'categoryorder':'total ascending'})
    # Exibe o gráfico
    fig_feat_rf.show()
else:
    print("Erro: Dados de importância das features do Random Forest não estão disponíveis para gerar a visualização.")


# --- Visualização Interativa da Importância das Features (XGBoost) ---
print("\nGerando Gráfico Interativo de Importância das Features (XGBoost)...")
# Verifica se os dados de importância das features do XGBoost estão disponíveis
if 'sorted_feature_importances_xgb' in locals():
    # Criar um DataFrame com as top 10 features e suas descrições para usar no gráfico
    top_10_xgb = sorted_feature_importances_xgb.head(10).reset_index()
    top_10_xgb.columns = ['Feature', 'Importancia']
    # Mapeia as features para suas descrições, usando um fallback se a descrição não for encontrada
    top_10_xgb['Descricao'] = top_10_xgb['Feature'].map(feature_descriptions).fillna('Descrição não disponível')

    # Cria o gráfico de barras interativo para o XGBoost
    fig_feat_xgb = px.bar(top_10_xgb,
                         x='Importancia', # Eixo X: o valor da importância
                         y='Feature',     # Eixo Y: o nome da feature
                         orientation='h', # Barras horizontais
                         labels={'Importancia':'Importância da Feature', 'Feature':'Feature'}, # Rótulos dos eixos
                         title='Top 10 Importância das Features Interativa - XGBoost', # Título do gráfico
                         hover_data={'Descricao': True, 'Importancia':':.4f'}) # Adiciona Descricao e Importancia formatada ao hover

    # Ordena as barras pelo valor da importância
    fig_feat_xgb.update_layout(yaxis={'categoryorder':'total ascending'})
    # Exibe o gráfico
    fig_feat_xgb.show()
else:
    print("Erro: Dados de importância das features do XGBoost não estão disponíveis para gerar a visualização.")

print("\nEtapa de visualização interativa concluída.")

"""## Avaliar modelos

### Subtask:
Avaliar o desempenho dos modelos nos dados de teste utilizando métricas como Acurácia, Precisão, Recall e F1-score, e gerar a matriz de confusão.

**Reasoning**:
Evaluate the performance of the trained Random Forest and XGBoost models on the test set (`X_test`, `y_test`) using key classification metrics and generate confusion matrices to understand their performance in detail. This step follows directly after model training.

## Visualizar Importância das Features

### Subtask:
Visualizar a importância das features para cada modelo treinado (Random Forest e XGBoost).

**Reasoning**:
Visualize the feature importance scores from the trained Random Forest and XGBoost models. This helps in understanding which features contribute most significantly to the model's predictions and provides insights into the underlying data. This step follows model training.

## Finalizar MVP

### Subtask:
Summarize the key findings, insights, and next steps for the MVP.

**Reasoning**:
To finalize the MVP as per the plan, it's important to summarize the work done, the key findings from the data analysis and model evaluation, and outline the next steps for future development and presentation. This provides a clear overview of the project's status and future direction.

**Summary of MVP Development:**

The initial goal of developing a supervised machine learning MVP to classify exoplanet candidates using the Kepler Objects of Interest dataset has been partially achieved.

**Accomplishments:**

*   Necessary libraries for data manipulation, machine learning, and visualization were imported.
*   The Kepler Objects of Interest cumulative dataset was successfully loaded from a local file (addressing previous download issues).
*   The data underwent essential preprocessing steps, including selecting relevant features, handling missing values, encoding the target variable ('koi_disposition'), and normalizing numerical features.
*   The preprocessed data was split into stratified training and testing sets.
*   Random Forest and XGBoost classifiers were successfully trained on the training data.
*   Model performance was evaluated using accuracy, precision, recall, and F1-score, and confusion matrices were generated.
*   Feature importance for both models was visualized.

**Key Findings from Analysis and Evaluation:**

Aqui está uma análise dos resultados e da importância das features para torná-los mais compreensíveis:

**Desempenho dos Modelos (Random Forest vs. XGBoost):**

*   Ambos os modelos apresentaram um desempenho razoavelmente bom na classificação dos candidatos a exoplanetas, com **Acurácias em torno de [Inserir Acurácia RF]% (Random Forest) e [Inserir Acurácia XGB]% (XGBoost)**.
*   As matrizes de confusão mostram como cada modelo se saiu na classificação das três categorias: CONFIRMED (Planeta Confirmado), CANDIDATE (Candidato) e FALSE POSITIVE (Falso Positivo).
    *   Você pode ver na diagonal principal da matriz quantos candidatos de cada classe foram classificados corretamente.
    *   Os números fora da diagonal principal mostram os erros (por exemplo, quantos Planetas Confirmados foram classificados incorretamente como Falsos Positivos).
*   As métricas de Precisão, Recall e F1-score (weighted) fornecem uma visão mais detalhada, especialmente considerando que as classes podem não estar perfeitamente balanceadas no conjunto de dados.

**Importância das Features:**

*   Os gráficos de barras "Top 10 Importância das Features" para Random Forest e XGBoost indicam quais características dos candidatos a exoplanetas foram mais relevantes para cada modelo na tomada de decisão.
*   Note que alguns features aparecem como muito importantes para ambos os modelos. Por exemplo:
    *   `koi_fpflag_co`, `koi_fpflag_nt`, `koi_fpflag_ss`, `koi_fpflag_ec`: Estas são flags de falso positivo geradas por análises anteriores. É esperado que sejam muito importantes, pois indicam características que geralmente levam à classificação como Falso Positivo.
    *   `koi_model_snr`: A Razão Sinal-Ruído do modelo de trânsito. Um SNR alto geralmente indica um sinal de trânsito mais claro e menos propenso a ser ruído ou um artefato.
    *   `koi_prad` (Raio Planetário) e `koi_depth` (Profundidade do Trânsito): Características físicas do planeta e do trânsito que são fundamentais para a identificação de exoplanetas.
*   Observar as diferenças nas features mais importantes entre Random Forest e XGBoost pode fornecer insights sobre como cada algoritmo aborda o problema.

**Em resumo:** Os modelos conseguiram aprender padrões nos dados do Kepler, com as flags de falso positivo e características do trânsito sendo as mais influentes para a classificação.

**Next Steps:**

1.  **Preencher os Valores Analisados**: Substitua os placeholders `[Inserir Acurácia RF]%` e `[Inserir Acurácia XGB]%` no resumo acima com os valores reais obtidos na execução da célula de avaliação. Analise as matrizes de confusão e comente brevemente sobre os principais acertos e erros para cada modelo.
2.  **Refinement (Optional)**: Consider hyperparameter tuning for the models to potentially improve performance.
3.  **Prediction Function**: Develop a function to take new data and make predictions using the trained models.
4.  **Expand Data Sources**: Plan for the integration of TESS and K2 datasets.
5.  **Comprehensive Documentation**: Enhance code comments and add markdown explanations throughout the notebook for the hackathon presentation.
6.  **Presentation Preparation**: Organize the notebook logically and ensure visualizations are clear for the presentation.

## Visualizar Importância das Features

### Subtask:
Visualizar a importância das features para cada modelo treinado (Random Forest e XGBoost).

**Reasoning**:
Visualize the feature importance scores from the trained Random Forest and XGBoost models. This helps in understanding which features contribute most significantly to the model's predictions and provides insights into the underlying data. This step follows model training.
"""

# Passo 1: Obter a importância das features do modelo Random Forest.
if 'rf_model' in locals() and 'X_train' in locals():
    print("Visualizando importância das features para Random Forest...")
    feature_importances_rf = pd.Series(rf_model.feature_importances_, index=X_train.columns)

    # Passo 2: Classificar as features por importância em ordem decrescente.
    sorted_feature_importances_rf = feature_importances_rf.sort_values(ascending=False)

    # Passo 3: Exibir as 10 features mais importantes (ou ajuste conforme necessário).
    print("\nTop 10 Features Mais Importantes (Random Forest):")
    display(sorted_feature_importances_rf.head(10))

    # Passo 4: Visualizar a importância das features.
    plt.figure(figsize=(10, 6))
    sns.barplot(x=sorted_feature_importances_rf.head(10), y=sorted_feature_importances_rf.head(10).index, palette='viridis')
    plt.xlabel('Importância da Feature')
    plt.ylabel('Feature')
    plt.title('Top 10 Importância das Features - Random Forest')
    plt.tight_layout()
    plt.show()
else:
    print("Erro: O modelo Random Forest ou os dados de treino não estão disponíveis para visualizar a importância das features.")


# Passo 5: Obter a importância das features do modelo XGBoost.
if 'xgb_model' in locals() and 'X_train' in locals():
    print("\nVisualizando importância das features para XGBoost...")
    feature_importances_xgb = pd.Series(xgb_model.feature_importances_, index=X_train.columns)

    # Passo 6: Classificar as features por importância em ordem decrescente.
    sorted_feature_importances_xgb = feature_importances_xgb.sort_values(ascending=False)

    # Passo 7: Exibir as 10 features mais importantes (ou ajuste conforme necessário).
    print("\nTop 10 Features Mais Importantes (XGBoost):")
    display(sorted_feature_importances_xgb.head(10))


    # Passo 8: Visualizar a importância das features.
    plt.figure(figsize=(10, 6))
    sns.barplot(x=sorted_feature_importances_xgb.head(10), y=sorted_feature_importances_xgb.head(10).index, palette='viridis')
    plt.xlabel('Importância da Feature')
    plt.ylabel('Feature')
    plt.title('Top 10 Importância das Features - XGBoost')
    plt.tight_layout()
    plt.show()
else:
    print("Erro: O modelo XGBoost ou os dados de treino não estão disponíveis para visualizar a importância das features.")

from google.colab import drive
drive.mount('/content/drive')

"""## Finalizar MVP

### Subtask:
Summarize the key findings, insights, and next steps for the MVP.

## Melhorias Potenciais no Código

Com base nas execuções atuais e nos dados do `koi-cumulative.csv`, aqui estão algumas sugestões para melhorar o código e o MVP:

1.  **Tratamento do `FutureWarning` em `fillna(inplace=True)`**:
    *   A execução da célula de pré-processamento gerou um `FutureWarning` relacionado ao uso de `inplace=True` com `fillna`. Embora ainda funcione na versão atual, é recomendado evitar `inplace=True` em futuras versões do pandas.
    *   **Melhoria**: Substituir `df_processed[col].fillna(df_processed[col].mean(), inplace=True)` por `df_processed[col] = df_processed[col].fillna(df_processed[col].mean())`.

2.  **Tratamento de Valores Ausentes Mais Sofisticado**:
    *   Atualmente, preenchemos todos os valores numéricos ausentes com a média. Embora simples, nem sempre é a melhor estratégia.
    *   **Melhoria**: Explorar outras técnicas de imputação, como a mediana, a moda, ou métodos mais avançados como imputação por k-NN ou imputação baseada em modelos (por exemplo, usando um modelo para prever os valores ausentes com base em outras features). A escolha pode depender da distribuição de cada feature.

3.  **Seleção de Features Baseada na Importância**:
    *   Visualizamos a importância das features, e notamos que algumas features têm uma importância muito baixa para ambos os modelos.
    *   **Melhoria**: Considerar remover features de baixa importância para simplificar o modelo, reduzir o ruído e potencialmente melhorar o desempenho e a velocidade de treinamento. Podemos experimentar diferentes limiares de importância.

4.  **Otimização de Hiperparâmetros**:
    *   Os modelos Random Forest e XGBoost foram treinados com parâmetros padrão. A otimização de hiperparâmetros pode melhorar significativamente o desempenho do modelo.
    *   **Melhoria**: Implementar técnicas de otimização como Grid Search ou Random Search, possivelmente com validação cruzada, para encontrar a melhor combinação de hiperparâmetros para cada modelo.

5.  **Explorar Outros Modelos**:
    *   Embora Random Forest e XGBoost sejam poderosos, outros algoritmos podem ser adequados para este problema.
    *   **Melhoria**: Experimentar outros modelos de classificação, como Support Vector Machines (SVM), K-Nearest Neighbors (KNN), ou até mesmo redes neurais simples, para comparar o desempenho.

6.  **Avaliação Mais Detalhada**:
    *   As métricas atuais (Acurácia, Precisão, Recall, F1-score) e a matriz de confusão fornecem uma boa visão geral.
    *   **Melhoria**: Para um problema de classificação multiclasse como este, métricas por classe (precision, recall, f1-score para cada uma das classes: CONFIRMED, CANDIDATE, FALSE POSITIVE) podem fornecer insights mais detalhados sobre onde cada modelo acerta e erra. A curva ROC e a Área sob a Curva (AUC) para multiclasse também podem ser úteis, embora exijam uma abordagem one-vs-rest ou one-vs-one.

7.  **Modularização com Funções**:
    *   Embora tenhamos combinado algumas etapas em células, podemos modularizar ainda mais o código usando funções para pré-processamento, treinamento e avaliação.
    *   **Melhoria**: Criar funções reutilizáveis torna o código mais limpo, mais fácil de testar e manter.

8.  **Tratamento da Coluna 'kepmag'**:
    *   A coluna 'kepmag' foi identificada como ausente durante a seleção inicial de features. Pode ser útil investigar por que ela estava faltando e se há dados alternativos ou uma forma de incluí-la, caso seja considerada uma feature importante.

Implementar essas melhorias pode levar o MVP para o próximo nível em termos de desempenho e robustez.

**Reasoning**:
To finalize the MVP as per the plan, it's important to summarize the work done, the key findings from the data analysis and model evaluation, and outline the next steps for future development and presentation. This provides a clear overview of the project's status and future direction.

**Summary of MVP Development:**

The initial goal of developing a supervised machine learning MVP to classify exoplanet candidates using the Kepler Objects of Interest dataset has been partially achieved.

**Accomplishments:**

*   Necessary libraries for data manipulation, machine learning, and visualization were imported.
*   The Kepler Objects of Interest cumulative dataset was successfully loaded from a local file (addressing previous download issues).
*   The data underwent essential preprocessing steps, including selecting relevant features, handling missing values, encoding the target variable ('koi_disposition'), and normalizing numerical features.
*   The preprocessed data was split into stratified training and testing sets.
*   Random Forest and XGBoost classifiers were successfully trained on the training data.
*   Model performance was evaluated using accuracy, precision, recall, and F1-score, and confusion matrices were generated.
*   Feature importance for both models was visualized.

**Key Findings from Analysis and Evaluation (based on executed cells):**

*   [Insert analysis of evaluation metrics from cell `cecf8c1a` here - e.g., which model had higher accuracy, precision for specific classes, etc.]
*   [Insert interpretation of feature importance from cell `c56f1167` here - e.g., which features were most important for each model, and potential reasons why.]

**Next Steps:**

1.  **Detailed Analysis and Interpretation**: Provide a more in-depth analysis of the evaluation results and feature importance findings within this summary or in dedicated markdown cells.
2.  **Refinement (Optional)**: Consider hyperparameter tuning for the models to potentially improve performance.
3.  **Prediction Function**: Develop a function to take new data and make predictions using the trained models.
4.  **Expand Data Sources**: Plan for the integration of TESS and K2 datasets.
5.  **Comprehensive Documentation**: Enhance code comments and add markdown explanations throughout the notebook for the hackathon presentation.
6.  **Presentation Preparation**: Organize the notebook logically and ensure visualizations are clear for the presentation.
"""